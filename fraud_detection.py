# -*- coding: utf-8 -*-
"""Fraud Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t5lSykRHDyFY_BEcFgoqI5COoJorZ-pk

Fraud Detection Data set: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?select=creditcard.csv

The dataset contains transactions made by credit cards in September 2013 by European cardholders.

This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.


 due to confidentiality issues, we cannot provide the original features and more background information about the data

  Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
"""

import pandas as pd

#read fraud detection data set
data=pd.read_csv("/content/creditcard.csv")

#checke the data set shape
data.shape

#The data set columns names
data.columns

"""Columns:Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10,
       V11, V12, V13, V14, V15, V16, V17, V18, V19, V20,
       V21, V22, V23, V24, V25, V26, V27, V28, Amount.


Target:Class(0 = Legitimate Transaction, 1 = Fraudulent Transaction)
"""

#show the first 10 records of dataset
data.head(10)

#show the last 10 records of data set
data.tail(10)

#show general information about data set
data.info()

""" Dataset Overview:

Total Entries: 284,807 transactions

Columns: 31 features (including the target variable Class)

Features:
Time: Represents the time elapsed in seconds from the first transaction.
V1 to V28: These are anonymized features extracted using PCA (Principal Component Analysis).

Amount: Represents the transaction amount.

Class: The target variable (0 = Legitimate Transaction, 1 = Fraudulent Transaction).

 Initial Observations:

 No Missing Values

Every column has 284,807 non-null values, meaning no missing data exists.
"""

#display statistical information about the data set
data.describe()

"""General Observations:

-->Class Imbalance:


-->The Class column shows a mean of 0.001727, meaning only 0.17% of transactions are fraudulent.

This confirms that the dataset is highly imbalanced.


-Transaction Amount (Amount) Distribution:
Mean: 88.35
Standard Deviation: 250.12
Min: 0.00
Max: 25,691.16
This suggests that transactions have a wide range of values,

-->Time Feature (Time):
The dataset spans from 0 to 172,792 seconds (~48 hours).
Transactions occur continuously over time, so analyzing time-based fraud patterns could be useful.


-->PCA Features (V1 to V28):
Mean values are close to 0, as expected from PCA-transformed data.
Standard deviations vary, with some features having significantly larger ranges (V6 max = 73.30, V7 max = 120.59).

-->Some features have negative minimum values, indicating that they might represent different types of transaction behaviors.

-->Features like V1, V2, V3, etc., have wide ranges, which may indicate the presence of outliers that need further investigation.


-->Potential Outliers in PCA Features:
V6 and V7 have very large max values, suggesting possible outliers that could impact model performance.

"""

#check  for duplicate values in a data set
data.duplicated().sum()

"""The data set contain 1081 duplicates value  that we will remove"""

#remove duplicated values
data.drop_duplicates(inplace=True)

data.duplicated().sum()

#check for null values in a data set
data.isnull().sum()

"""  No nulls in the data set"""

#check unique values in the data set
for column in data.columns:
    unique_values = data[column].nunique()
    print(f"Feature '{column}' has {unique_values} unique values.")

"""
Display Data Set Profiling"""

pip install ydata-profiling

from ydata_profiling import ProfileReport
# Create report
profile = ProfileReport(data, explorative=True)
# Save the report to HTML file
profile.to_file("pandas_profiling_report.html")

from google.colab import files

files.download("/content/pandas_profiling_report.html")

"""Class Distribution"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6,4))
sns.countplot(x=data['Class'], palette=['green', 'red'])
plt.title("Transaction Class Distribution")
plt.xlabel("Class (0 = Legit, 1 = Fraud)")
plt.ylabel("Count")
plt.show()

"""The number of class 1 samples should be increased.

Transaction Amount Distribution
"""

plt.figure(figsize=(10,5))
sns.histplot(data['Amount'], bins=50, kde=True, color='blue')
plt.title("Transaction Amount Distribution")
plt.xlabel("Transaction Amount")
plt.ylabel("Frequency")
plt.xlim(0, 3000)
plt.show()

"""-->Insights from the Transaction Amount Distribution Plot:

-->Transactions are mostly low in value

We see that most of the financial transactions fall within the range of very small amounts (close to 0 - 500).
This means that the vast majority of transactions are for low amounts, which is common in daily card use.

-->Highly Skewed Distribution

The figure shows that the distribution of amounts is unbalanced and strongly skewed to the left (Right-skewed distribution).
Very few transactions are for large amounts, but most are very small.

-->Potential for Fraudulent Activities in High Amounts?

If fraudulent transactions are typically of high amounts, then this plot may indicate that any transaction with a high amount may be more susceptible to scrutiny.

ompare fraudulent and non-fraudulent transactions
"""

sns.boxplot(x=data["Class"], y=data["Amount"], palette=["green", "red"])

"""Insights from the Boxplot of Transaction Amount vs Class

- Legitimate Transactions Have Higher Amount Variability

Class 0 (Normal Transactions) has a large set of outliers, and there are transactions with amounts greater than 25,000!

This indicates that normal transactions may involve large purchases, making it difficult to judge fraud based on the amount alone.

Fraudulent Transactions Are Usually Lower in Amount


Class 1 (fraudulent transactions) generally has smaller amounts, with no large transactions compared to the normal class.

This suggests that fraud is often occurring with relatively low amounts, perhaps to avoid detection by security systems.
"""

sns.histplot(data[data["Class"] == 1]["Amount"], bins=50, kde=True, color="red")
plt.title("Fraudulent Transaction Amount Distribution")
plt.show()

"""The distribution is strongly skewed to the right

Most fraudulent transactions are for small amounts, and the incidence gradually decreases as the amount increases.
There are very few cases for large amounts, suggesting that fraudsters prefer to keep transactions small to avoid suspicion.
Clear peak at very small amounts

We see a sharp peak at low amounts, meaning that most fraudulent transactions fall within this range.
This may be due to fraudsters trying to pass the transactions off without attracting attention.
Long tail to the right

There are fraudulent transactions for large amounts, but they are rare.
These transactions may represent larger fraud attempts, but they are less common.

Fraud Transactions Over Time
"""

plt.figure(figsize=(10,5))
sns.histplot(data[data["Class"] == 1]["Time"], bins=50, color='red', label="Fraud", kde=True)
sns.histplot(data[data["Class"] == 0]["Time"], bins=50, color='green', label="Legit", kde=True)
plt.title("Transaction Distribution Over Time")
plt.xlabel("Time (Seconds)")
plt.ylabel("Frequency")
plt.legend()
plt.show()

"""1. Distribution of transactions over time
The graph shows the distribution of transactions over time in seconds, with a distinction between fraudulent transactions (in red) and legitimate transactions (in green).
Most legitimate transactions follow a cyclical pattern with clear peaks and troughs, indicating the influence of time factors (such as business hours or periods of activity).
Fraudulent transactions are almost invisible in the graph, indicating that they are very few compared to legitimate transactions.
2. Activity peaks and their association with fraud
There are two main patterns of activity that appear as clear waves in the data, with peaks around 75,000 seconds and 150,000 seconds.
This may reflect peak periods for legitimate transactions, such as business hours or active shopping times.
Fraudulent transactions appear to be very sparsely distributed, which may indicate that fraudsters are trying to blend into normal periods to avoid suspicion.
3. Distribution of fraudulent transactions
Although fraudulent transactions are very few, there may be a temporal pattern to their occurrence.
You can focus on periods of low activity (times when legitimate transactions are low), when there may be a greater chance of detecting fraud.

Correlation Heatmap
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(20,16))
sns.heatmap(data.corr(), cmap="coolwarm", annot=True, fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

"""We identify fraudulent transactions that are above a certain threshold (such as the 95th percentile of fraudulent amounts):"""

# Calculate the maximum fraudulent amounts (e.g., the top 5% of amounts)
fraud_df = data[data["Class"] == 1]
threshold = fraud_df["Amount"].quantile(0.95)
# Extracting large fraudulent transactions
high_amount_fraud = fraud_df[fraud_df["Amount"] > threshold]
# Show general statistics
print(high_amount_fraud.describe())

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.kdeplot(high_amount_fraud["Amount"], fill=True, color="red", alpha=0.6)
plt.xlabel("Transaction Amount")
plt.ylabel("Density")
plt.title("Distribution of High-Amount Fraudulent Transactions")
plt.show()

"""Chart Analysis:
1. Distribution of high-value fraudulent transactions
This chart shows the distribution of high-value fraudulent transactions, where the density curve follows an asymmetric distribution skewed to the right.
The main peak of the distribution is approximately between 700 and 1000, meaning that most high-value fraudulent transactions are concentrated within this range.
After the peak, the curve begins to gradually decline, indicating that there are fraudulent transactions with larger amounts but they are less common.
2. Long Tail
The long tail indicates that there are a few fraudulent transactions with very large amounts up to more than 2500.
These cases may be large fraudulent attempts to withdraw large amounts, which may require additional scrutiny, since they are less common but more serious.
Conclusions and Recommendations:
- Focus on the range between 700 - 1000 in fraud detection systems, where most high-value fraudulent transactions occur.
- Monitor transactions above 2000 in particular, as they may be part of large or organized fraudulent operations.

- Analyze the relationship between these high amounts and other factors such as geographic location or timing of the transaction to detect any specific patterns.

Does it happen at certain hours?
"""

plt.figure(figsize=(12, 6))
sns.histplot(high_amount_fraud["Time"] % 86400 / 3600, bins=24, kde=True, color="red")
plt.xlabel("Hour of Day")
plt.ylabel("Transaction Count")
plt.title("High-Amount Fraudulent Transactions by Hour")
plt.xticks(range(0, 24))
plt.show()

"""1. Distribution of high-value fraudulent transactions by hour
The graph shows the distribution of high-value fraudulent transactions throughout the day.
The horizontal axis represents the hours of the day (from 0 to 23), while the vertical axis represents the number of fraudulent transactions.
It appears that some hours have higher fraudulent activity than others, which may indicate periods targeted by fraudsters.
2. Fraud peak hours
There is a noticeable increase in the number of fraudulent transactions during 19:00 (7pm), which may indicate a sensitive period.
Other peaks are also observed at 5:00 am, 11:00 am, and 17:00 pm, which may indicate that fraudsters target these periods.
Between 12:00 and 16:00 there is relative stability, which may indicate a decrease in fraudulent activity during this period.

Correlation analysis between fraudulent amounts and other features
"""

correlation = high_amount_fraud.corr()["Amount"].sort_values(ascending=False)
print(correlation)

"""1. Most significant positively correlated variables (more correlated with high amount):
- V9 (0.326): This variable is the most correlated with amount, meaning it can have a significant impact on determining the transaction size.
- V24 (0.312): The second strongest positive correlation, suggesting its potential use in fraud detection models when dealing with large amounts.
- V14 (0.170): Still has a positive correlation but is less strong compared to V9 and V24.
- V20, V18, V16, V27: All of them are positively correlated with amount but to a lesser degree.
--Possible explanation:
These variables may be associated with specific patterns of fraud, such as repeating transactions of certain amounts or the presence of abnormal behavior in financial values.

2. Most significant negatively correlated variables (most adversely affecting amount):
- V2 (-0.437): The strongest negative correlation, meaning that high values â€‹â€‹of this variable are associated with low transaction amounts.
- V22 (-0.405), V8 (-0.391), V23 (-0.317): All have a strong negative correlation, suggesting that they may be indicators of low-amount transactions.
- V26 (-0.150): Its effect is smaller but still negative, meaning that it can be useful for pattern analysis.
- Time (-0.025): It has no significant effect, meaning that the transaction amount is not significantly affected by time.
- Possible explanation:
Fraudulent transactions with high values â€‹â€‹may be associated with a decrease in some of these variables, which may be due to financial manipulation by fraudsters.

3. Less influential variables (weakly correlated):
V12, V4, V28, V6, V17: All are close to zero, meaning that their effect on the fraudulent transaction amount is very weak or unclear.

Comparing large vs. small fraudulent transactions
"""

plt.figure(figsize=(12, 6))
sns.boxplot(x=fraud_df["Amount"] > threshold, y=fraud_df["Amount"], showfliers=True)
plt.xlabel("Is High Amount Fraud")
plt.ylabel("Transaction Amount")
plt.title("Comparison of Fraudulent Transaction Amounts")
plt.xticks([0, 1], ["Small Fraud", "High Fraud"])
plt.show()

"""Box analysis of fraudulent transaction amounts
- Notes from the chart:
Small Fraud:

Most fraudulent transactions fall within a very low range.
There are several outliers that exceed the upper limit but do not reach very high amounts.
The distribution is very dense at the low values, meaning that the vast majority of fraudulent transactions are for small amounts.
High Fraud:

A much wider range of transaction amounts is shown.
The median (line inside the box) is much higher compared to small fraud, meaning that most of these transactions occur for large amounts.
The spread of the data is much larger, indicating a variety of values â€‹â€‹between 500 and over 2000.
There are no clear outliers as most of the values â€‹â€‹are high in nature.
- Key takeaways:
--> Most fraudulent transactions occur for very small amounts, but there is a class of fraudulent transactions that target clearly high amounts.
--> Extreme values â€‹â€‹in petty fraud may be transitional cases between the two categories, which requires further investigation.
--> Major fraud should be treated differently in fraud detection models, because it has a different behavior than petty fraud.
"""

data.to_csv("/content/Silver_creditcard.csv",index=False)

"""#Data preprocessing"""

Silver_data=pd.read_csv("/content/Silver_creditcard.csv")

Silver_data.duplicated().sum()

Silver_data.isnull().sum()

Silver_data = Silver_data.dropna()

"""Handling outliers"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 8))

sns.boxplot(data=Silver_data)

plt.xticks(rotation=90)

plt.title('Box Plot of all features to see outliers')

plt.grid()

plt.show()

Q1 = Silver_data.quantile(0.25)
Q3 = Silver_data.quantile(0.75)
IQR = Q3 - Q1
outliers = ((Silver_data< (Q1 - 1.5 * IQR)) | (Silver_data> (Q3 + 1.5 * IQR)))

outliers_count = outliers.sum()

print(outliers_count)

import pandas as pd
import numpy as np

def replace_outliers_with_median(Silver_data):
    outlier_counts_before = {}
    outlier_counts_after = {}

    for column in Silver_data.select_dtypes(include=[np.number]):
        Q1 = Silver_data[column].quantile(0.25)
        Q3 = Silver_data[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        outliers = (Silver_data[column] < lower_bound) | (Silver_data[column] > upper_bound)
        num_outliers_before = outliers.sum()

        median_value = Silver_data[column].median()
        Silver_data.loc[outliers, column] = median_value

        num_outliers_after = ((Silver_data[column] < lower_bound) | (Silver_data[column] > upper_bound)).sum()

        outlier_counts_before[column] = num_outliers_before
        outlier_counts_after[column] = num_outliers_after

    return Silver_data, outlier_counts_before, outlier_counts_after

Silver_data, outliers_before, outliers_after = replace_outliers_with_median(Silver_data)

for col in outliers_before:
    print(f"Feature '{col}':")
    print(f"  Outliers before replacement: {outliers_before[col]}")
    print(f"  Outliers after replacement: {outliers_after[col]}")
    print("-" * 40)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(15, 8))

sns.boxplot(data=Silver_data)

plt.xticks(rotation=90)

plt.title('Box Plot of all features to see outliers')

plt.grid()

plt.show()

"""Standaraization"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
Silver_data_scaled = pd.DataFrame(scaler.fit_transform(Silver_data), columns=Silver_data.columns)

print(Silver_data_scaled.head())

"""Data balance"""

print(data['Class'].value_counts())
Silver_data = data.copy()

print(Silver_data['Class'].isnull().sum())

Silver_data = Silver_data.dropna()

from imblearn.over_sampling import SMOTE
import pandas as pd

if 'Silver_data' not in locals():
    print("Error: Silver_data is not defined. Please load the dataset first.")


X = Silver_data.drop(columns=['Class'])
y = Silver_data['Class']


smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)


Silver_data_resampled = pd.DataFrame(X_resampled, columns=X.columns)
Silver_data_resampled['Class'] = y_resampled


print(Silver_data_resampled['Class'].value_counts())

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.countplot(x=Silver_data_resampled['Class'], palette=['green', 'red'])
plt.title("Transaction Class Distribution After SMOTE")
plt.xlabel("Class (0 = Legit, 1 = Fraud)")
plt.ylabel("Count")
plt.show()

"""Feature importance"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

X = Silver_data.drop(columns=['Class'])
y = Silver_data['Class']

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

feature_importance = rf.feature_importances_

importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

print(importance_df)

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df, palette='viridis')
plt.title('Feature Importance using Random Forest')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.grid()
plt.show()

Silver_data.to_csv("/content/gold_creditcard.csv",index=False)

"""#Modeling"""

gold_data=pd.read_csv("/content/gold_creditcard.csv")

"""Split the data set into train & test"""

from sklearn.model_selection import train_test_split


X = gold_data.drop(columns=['Class'])
y = gold_data['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, precision_recall_curve, auc, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Train Logistic Regression
log_reg = LogisticRegression(class_weight='balanced', max_iter=1000)
log_reg.fit(X_train, y_train)

# Predictions
y_train_pred = log_reg.predict(X_train)
y_test_pred = log_reg.predict(X_test)

# Compute F1-Score
f1_train_log = f1_score(y_train, y_train_pred)
f1_test_log= f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, log_reg.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_log:.4f}")
print(f"F1-Score (Test): {f1_test_log:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""SVM"""

from sklearn.svm import SVC
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Train SVM
svm = SVC(probability=True, kernel='linear')  # Use probability=True to enable predict_proba()
svm.fit(X_train, y_train)

# Predictions
y_train_pred = svm.predict(X_train)
y_test_pred = svm.predict(X_test)

# Compute F1-Score
f1_train_svm = f1_score(y_train, y_train_pred)
f1_test_svm = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, svm.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_svm:.4f}")
print(f"F1-Score (Test): {f1_test_svm :.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - SVM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

# Train Naive Bayes
nb = GaussianNB()
nb.fit(X_train, y_train)

# Predictions
y_train_pred = nb.predict(X_train)
y_test_pred = nb.predict(X_test)

# Compute F1-Score
f1_train_nb = f1_score(y_train, y_train_pred)
f1_test_nb= f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, nb.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_nb :.4f}")
print(f"F1-Score (Test): {f1_test_nb:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Train KNN
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predictions
y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

# Compute F1-Score
f1_train_knn = f1_score(y_train, y_train_pred)
f1_test_knn = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, knn.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_knn:.4f}")
print(f"F1-Score (Test): {f1_test_knn:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - KNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

""" Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

# Train Decision Tree
dt = DecisionTreeClassifier(random_state=42, class_weight='balanced')
dt.fit(X_train, y_train)

# Predictions
y_train_pred = dt.predict(X_train)
y_test_pred = dt.predict(X_test)

# Compute F1-Score
f1_train_dt= f1_score(y_train, y_train_pred)
f1_test_dt = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, dt.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_dt:.4f}")
print(f"F1-Score (Test): {f1_test_dt:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Decision Tree")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf.fit(X_train, y_train)

# Predictions
y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

# Compute F1-Score
f1_train_rf = f1_score(y_train, y_train_pred)
f1_test_rf= f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, rf.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_rf:.4f}")
print(f"F1-Score (Test): {f1_test_rf:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Ensamble learing(Logistic,SVM,NB,KNN)"""

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve, auc

# Definition of basic models
base_learners = [
    ('logreg', LogisticRegression(class_weight='balanced', max_iter=1000)),
    ('svm', SVC(probability=True, kernel='linear')),
    ('nb', GaussianNB()),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]

# Definition of the final model (Meta-model)
meta_model = LogisticRegression()

# Create StackingClassifier
stacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_model)

# Model training
stacking_model.fit(X_train, y_train)

# Predicting results
y_train_pred = stacking_model.predict(X_train)
y_test_pred = stacking_model.predict(X_test)

# F1-Score Calculation
f1_train_stack = f1_score(y_train, y_train_pred)
f1_test_stack = f1_score(y_test, y_test_pred)

# AUPRC account
precision, recall, _ = precision_recall_curve(y_test, stacking_model.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print results
print(f"F1-Score (Train): {f1_train_stack:.4f}")
print(f"F1-Score (Test): {f1_test_stack:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Draw confusion matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Stacking Classifier")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

""" XGBoost"""

from xgboost import XGBClassifier

# Train XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train)

# Predictions
y_train_pred = xgb.predict(X_train)
y_test_pred = xgb.predict(X_test)

# Compute F1-Score
f1_train_xgb = f1_score(y_train, y_train_pred)
f1_test_xgb = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, xgb.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_xgb:.4f}")
print(f"F1-Score (Test): {f1_test_xgb :.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

""" LGBM Classifier"""

from lightgbm import LGBMClassifier
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Train LGBMClassifier
LGBMClassifier_ = LGBMClassifier(eval_metric='logloss')
LGBMClassifier_.fit(X_train, y_train)

# Predictions
y_train_pred = LGBMClassifier_.predict(X_train)
y_test_pred = LGBMClassifier_.predict(X_test)

# Compute F1-Score
f1_train_lgbm= f1_score(y_train, y_train_pred)
f1_test_lgbm= f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, LGBMClassifier_.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_lgbm:.4f}")
print(f"F1-Score (Test): {f1_test_lgbm:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - LGBMClassifier")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""MLP Classifier"""

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Train MLP
mlp = MLPClassifier(max_iter=1000, solver='adam', hidden_layer_sizes=(100,), random_state=42)
mlp.fit(X_train, y_train)

# Predictions
y_train_pred = mlp.predict(X_train)
y_test_pred = mlp.predict(X_test)

# Compute F1-Score
f1_train_mlp = f1_score(y_train, y_train_pred)
f1_test_mlp = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, mlp.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_mlp:.4f}")
print(f"F1-Score (Test): {f1_test_mlp:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - MLP")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Train Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb.fit(X_train, y_train)

# Predictions
y_train_pred = gb.predict(X_train)
y_test_pred = gb.predict(X_test)

# Compute F1-Score
f1_train_gb = f1_score(y_train, y_train_pred)
f1_test_gb = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, gb.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_gb:.4f}")
print(f"F1-Score (Test): {f1_test_gb:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Gradient Boosting")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""cat boost"""

pip install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Train CatBoost
catboost = CatBoostClassifier(iterations=500, learning_rate=0.1, depth=6, verbose=0)
catboost.fit(X_train, y_train)

# Predictions
y_train_pred = catboost.predict(X_train)
y_test_pred = catboost.predict(X_test)

# Compute F1-Score
f1_train_catboost = f1_score(y_train, y_train_pred)
f1_test_catboost = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, catboost.predict_proba(X_test)[:, 1])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_catboost:.4f}")
print(f"F1-Score (Test): {f1_test_catboost:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - CatBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""DNN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Create DNN_MODEL
DNN_MODEL = Sequential()

# Input Layer
DNN_MODEL.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=regularizers.l2(0.01)))

# BatchNormalization and Dropout layers
DNN_MODEL.add(BatchNormalization())
DNN_MODEL.add(Dropout(0.5))

# Hidden Layers
DNN_MODEL.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
DNN_MODEL.add(BatchNormalization())
DNN_MODEL.add(Dropout(0.5))

DNN_MODEL.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
DNN_MODEL.add(BatchNormalization())
DNN_MODEL.add(Dropout(0.5))

# Output Layer
DNN_MODEL.add(Dense(1, activation='sigmoid'))

# Compile the model
DNN_MODEL.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks for EarlyStopping and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Train the model
DNN_MODEL.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),
              callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = DNN_MODEL.predict(X_train)
y_test_pred = DNN_MODEL.predict(X_test)

# Convert predictions to binary labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-Score
f1_train_dnn = f1_score(y_train, y_train_pred)
f1_test_dnn = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, DNN_MODEL.predict(X_test)[:, 0])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_dnn:.4f}")
print(f"F1-Score (Test): {f1_test_dnn:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - DNN_MODEL")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""LSTM"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np  # Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy

# Create lstm_model
lstm_model = Sequential()

# Input Layer (LSTM)
lstm_model.add(LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True,
                    kernel_regularizer=regularizers.l2(0.01)))
lstm_model.add(BatchNormalization())
lstm_model.add(Dropout(0.5))

# Additional LSTM Layer
lstm_model.add(LSTM(64, return_sequences=False, kernel_regularizer=regularizers.l2(0.01)))
lstm_model.add(BatchNormalization())
lstm_model.add(Dropout(0.5))

# Dense Layer
lstm_model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
lstm_model.add(Dropout(0.5))

# Output Layer
lstm_model.add(Dense(1, activation='sigmoid'))

# Compile the model
lstm_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks for EarlyStopping and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Convert to NumPy arrays and reshape input data for LSTM
X_train_lstm = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_lstm = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

# Train the model
lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test),
               callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = lstm_model.predict(X_train_lstm)
y_test_pred = lstm_model.predict(X_test_lstm)

# Convert predictions to binary labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-Score
f1_train_lstm = f1_score(y_train, y_train_pred)
f1_test_lstm = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, lstm_model.predict(X_test_lstm)[:, 0])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_lstm:.4f}")
print(f"F1-Score (Test): {f1_test_lstm:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - lstm_model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""GRUS"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np  # Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy

# Create grus_model
grus_model = Sequential()

# Input Layer (GRU)
grus_model.add(GRU(128, input_shape=(X_train.shape[1], 1), return_sequences=True,
                   kernel_regularizer=regularizers.l2(0.01)))
grus_model.add(BatchNormalization())
grus_model.add(Dropout(0.5))

# Additional GRU Layer
grus_model.add(GRU(64, return_sequences=False, kernel_regularizer=regularizers.l2(0.01)))
grus_model.add(BatchNormalization())
grus_model.add(Dropout(0.5))

# Dense Layer
grus_model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))
grus_model.add(Dropout(0.5))

# Output Layer
grus_model.add(Dense(1, activation='sigmoid'))

# Compile the model
grus_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks for EarlyStopping and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Convert to NumPy arrays and reshape input data for GRU
X_train_gru = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_gru = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

# Train the model
grus_model.fit(X_train_gru, y_train, epochs=100, batch_size=32, validation_data=(X_test_gru, y_test),
               callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = grus_model.predict(X_train_gru)
y_test_pred = grus_model.predict(X_test_gru)

# Convert predictions to binary labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-Score
f1_train_gru = f1_score(y_train, y_train_pred)
f1_test_gru = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, grus_model.predict(X_test_gru)[:, 0])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_gru:.4f}")
print(f"F1-Score (Test): {f1_test_gru:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - grus_model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

""" Encoder-Decoder"""

from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np  # ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªÙŠØ±Ø§Ø¯ NumPy

# Encoder part of the model
encoder_inputs = Input(shape=(X_train.shape[1], 1))  # Adjust input shape according to your data
encoder_gru = GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(encoder_inputs)
encoder_gru = BatchNormalization()(encoder_gru)
encoder_gru = Dropout(0.5)(encoder_gru)

encoder_gru2 = GRU(64, return_sequences=False, kernel_regularizer=regularizers.l2(0.01))(encoder_gru)
encoder_gru2 = BatchNormalization()(encoder_gru2)
encoder_gru2 = Dropout(0.5)(encoder_gru2)

# Decoder part of the model (this is a simple decoder with Dense output)
decoder_dense = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01))(encoder_gru2)
decoder_dense = Dropout(0.5)(decoder_dense)
decoder_output = Dense(1, activation='sigmoid')(decoder_dense)

# Create the encoder-decoder model
encoder_decoder_model = Model(inputs=encoder_inputs, outputs=decoder_output)

# Compile the model
encoder_decoder_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks for EarlyStopping and ReduceLROnPlateau
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Convert to NumPy arrays and reshape input data for GRU (Encoder-Decoder)
X_train_encdec = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))  # Convert to NumPy and reshape
X_test_encdec = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))  # Convert to NumPy and reshape

# Train the model
encoder_decoder_model.fit(X_train_encdec, y_train, epochs=100, batch_size=32, validation_data=(X_test_encdec, y_test),
                          callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = encoder_decoder_model.predict(X_train_encdec)
y_test_pred = encoder_decoder_model.predict(X_test_encdec)

# Convert predictions to binary labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-Score
f1_train_encdec = f1_score(y_train, y_train_pred)
f1_test_encdec = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, encoder_decoder_model.predict(X_test_encdec)[:, 0])
auprc = auc(recall, precision)

# Print Results
print(f"F1-Score (Train): {f1_train_encdec:.4f}")
print(f"F1-Score (Test): {f1_test_encdec:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Encoder-Decoder Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Models Comparison"""

import matplotlib.pyplot as plt
import numpy as np

# F1-Scores for each model
models = ['Logistic Regression','SVM', 'Naive Bayes','knn ', 'Decision Tree', 'Random Forest','stacking_model', 'XGBoost','LGBMClassifier','MLP','Gradient Boosting ','cat boost','DNN_MODEL','LSTM','GRUS',"encoder_decoder_model"]
f1_train_scores = [f1_train_log,f1_train_svm, f1_train_nb, f1_train_knn,f1_train_dt, f1_train_rf,f1_train_stack, f1_train_xgb ,f1_train_lgbm,f1_train_mlp,f1_train_gb,f1_train_catboost,f1_train_dnn,f1_train_lstm ,f1_train_gru,f1_train_encdec]
f1_test_scores = [f1_test_log,f1_train_svm, f1_test_nb,f1_test_knn, f1_test_dt, f1_test_rf, f1_test_stack,f1_test_xgb,f1_test_lgbm,f1_test_mlp,f1_test_gb,f1_test_catboost,f1_test_dnn,f1_test_lstm,f1_train_gru ,f1_train_encdec]

x = np.arange(len(models))  # the label locations

plt.figure(figsize=(15, 10))
plt.bar(x - 0.2, f1_train_scores, 0.4, label='Train F1-Score', color='blue')
plt.bar(x + 0.2, f1_test_scores, 0.4, label='Test F1-Score', color='red')

plt.xticks(x, models, rotation=30)
plt.xlabel('Models')
plt.ylabel('F1-Score')
plt.title('Comparison of F1-Scores on Train vs Test')
plt.legend()
plt.ylim(0, 1)  # Limit F1-score range between 0 and 1
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

"""Encoder-Decoder Model is the best model

#Fine Tunning
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np
from tensorflow.keras.layers import LeakyReLU

# Reduce learning rate for better convergence
learning_rate = 0.0005

# Define the optimized GRU-based model
encoder_inputs = Input(shape=(X_train.shape[1], 1))

# First GRU layer with increased units (256 instead of 128)
encoder_gru = GRU(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(encoder_inputs)
encoder_gru = BatchNormalization()(encoder_gru)
encoder_gru = Dropout(0.3)(encoder_gru)  # Reduce Dropout to preserve more information

# Second GRU layer with increased units (128 instead of 64)
encoder_gru2 = GRU(128, return_sequences=False, kernel_regularizer=regularizers.l2(0.01))(encoder_gru)
encoder_gru2 = BatchNormalization()(encoder_gru2)
encoder_gru2 = Dropout(0.3)(encoder_gru2)  # Reduce Dropout

# Fully connected layer with LeakyReLU for better activation handling
decoder_dense = Dense(64, kernel_regularizer=regularizers.l2(0.01))(encoder_gru2)
decoder_dense = LeakyReLU(alpha=0.1)(decoder_dense)  # Replace ReLU with LeakyReLU
decoder_dense = Dropout(0.3)(decoder_dense)

# Output layer (sigmoid for binary classification)
decoder_output = Dense(1, activation='sigmoid')(decoder_dense)

# Build the model
encoder_decoder_model = Model(inputs=encoder_inputs, outputs=decoder_output)

# Compile the model with a smaller learning rate
encoder_decoder_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])

# Define callbacks for early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)

# Convert data to NumPy arrays and reshape for GRU input
X_train_encdec = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_encdec = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

# Train the model with a larger batch size (64 instead of 32)
encoder_decoder_model.fit(X_train_encdec, y_train, epochs=50, batch_size=64, validation_data=(X_test_encdec, y_test),
                          callbacks=[early_stopping, reduce_lr])

# Make predictions
y_train_pred = encoder_decoder_model.predict(X_train_encdec)
y_test_pred = encoder_decoder_model.predict(X_test_encdec)

# Convert predictions to binary values
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-score
f1_train_encdec = f1_score(y_train, y_train_pred)
f1_test_encdec = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, encoder_decoder_model.predict(X_test_encdec)[:, 0])
auprc = auc(recall, precision)

# Print results
print(f"F1-Score (Train): {f1_train_encdec:.4f}")
print(f"F1-Score (Test): {f1_test_encdec:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot confusion matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Optimized GRU Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np

# Set optimized learning rate
learning_rate = 0.001

# Define the GRU-based model
encoder_inputs = Input(shape=(X_train.shape[1], 1))

# First GRU layer (Reduced units from 256 to 128)
encoder_gru = GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.001))(encoder_inputs)
encoder_gru = BatchNormalization()(encoder_gru)
encoder_gru = Dropout(0.4)(encoder_gru)  # Increased Dropout

# Second GRU layer (Reduced units from 128 to 64)
encoder_gru2 = GRU(64, return_sequences=False, kernel_regularizer=regularizers.l2(0.001))(encoder_gru)
encoder_gru2 = BatchNormalization()(encoder_gru2)
encoder_gru2 = Dropout(0.4)(encoder_gru2)  # Increased Dropout

# Fully connected layer with ReLU
decoder_dense = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001))(encoder_gru2)
decoder_dense = Dropout(0.4)(decoder_dense)  # Increased Dropout

# Output layer (sigmoid for binary classification)
decoder_output = Dense(1, activation='sigmoid')(decoder_dense)

# Build the model
encoder_decoder_model = Model(inputs=encoder_inputs, outputs=decoder_output)

# Compile the model
encoder_decoder_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])

# Define callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, min_lr=1e-6)

# Convert data to NumPy arrays and reshape
X_train_encdec = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_encdec = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

# Train the model with optimized batch size and epochs
encoder_decoder_model.fit(X_train_encdec, y_train, epochs=70, batch_size=32, validation_data=(X_test_encdec, y_test),
                          callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = encoder_decoder_model.predict(X_train_encdec)
y_test_pred = encoder_decoder_model.predict(X_test_encdec)

# Convert predictions to binary values
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-score
f1_train_encdec = f1_score(y_train, y_train_pred)
f1_test_encdec = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, encoder_decoder_model.predict(X_test_encdec)[:, 0])
auprc = auc(recall, precision)

# Print results
print(f"F1-Score (Train): {f1_train_encdec:.4f}")
print(f"F1-Score (Test): {f1_test_encdec:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot confusion matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Improved GRU Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np

# Define input shape based on training data
input_shape = (X_train.shape[1], 1)

# Encoder part of the model
encoder_inputs = Input(shape=input_shape)

# First GRU Layer with increased units
encoder_gru = GRU(256, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(encoder_inputs)
encoder_gru = BatchNormalization()(encoder_gru)
encoder_gru = Dropout(0.3)(encoder_gru)  # Lower dropout to retain more information

# Second GRU Layer with increased units
encoder_gru2 = GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01))(encoder_gru)
encoder_gru2 = BatchNormalization()(encoder_gru2)
encoder_gru2 = Dropout(0.3)(encoder_gru2)

# Third GRU Layer (Newly added)
encoder_gru3 = GRU(32, return_sequences=False, kernel_regularizer=regularizers.l2(0.01))(encoder_gru2)
encoder_gru3 = BatchNormalization()(encoder_gru3)
encoder_gru3 = Dropout(0.3)(encoder_gru3)

# Fully connected layers (Decoder part)
decoder_dense = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(encoder_gru3)
decoder_dense = Dropout(0.3)(decoder_dense)

# Output layer
decoder_output = Dense(1, activation='sigmoid')(decoder_dense)

# Create and compile the model
encoder_decoder_model = Model(inputs=encoder_inputs, outputs=decoder_output)
encoder_decoder_model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# Convert data to NumPy arrays and reshape for GRU
X_train_encdec = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_encdec = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

# Train the model with increased batch size
encoder_decoder_model.fit(X_train_encdec, y_train, epochs=150, batch_size=64, validation_data=(X_test_encdec, y_test),
                          callbacks=[early_stopping, reduce_lr])

# Predictions
y_train_pred = encoder_decoder_model.predict(X_train_encdec)
y_test_pred = encoder_decoder_model.predict(X_test_encdec)

# Convert predictions to binary labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

# Compute F1-Score
f1_train_encdec = f1_score(y_train, y_train_pred)
f1_test_encdec = f1_score(y_test, y_test_pred)

# Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, encoder_decoder_model.predict(X_test_encdec)[:, 0])
auprc = auc(recall, precision)

# Print results
print(f"F1-Score (Train): {f1_train_encdec:.4f}")
print(f"F1-Score (Test): {f1_test_encdec:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

# Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Encoder-Decoder Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import numpy as np

# Reduce Overfitting: Reduce GRU units and increase regularization
encoder_inputs = Input(shape=(X_train.shape[1], 1))

#  First GRU Layer (Reduced to 96 units)
encoder_gru = GRU(96, return_sequences=True, kernel_regularizer=regularizers.l2(0.02))(encoder_inputs)
encoder_gru = BatchNormalization()(encoder_gru)
encoder_gru = Dropout(0.5)(encoder_gru)  # Increased Dropout

#  Second GRU Layer (Reduced to 48 units)
encoder_gru2 = GRU(48, return_sequences=False, kernel_regularizer=regularizers.l2(0.02))(encoder_gru)
encoder_gru2 = BatchNormalization()(encoder_gru2)
encoder_gru2 = Dropout(0.5)(encoder_gru2)  # Increased Dropout

#  Fully Connected Layers (Regularized)
decoder_dense = Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02))(encoder_gru2)
decoder_dense = Dropout(0.4)(decoder_dense)  # Increased Dropout
decoder_output = Dense(1, activation='sigmoid')(decoder_dense)

#  Create the Model
encoder_decoder_model = Model(inputs=encoder_inputs, outputs=decoder_output)

#  Compile the Model (Reduced Learning Rate for Stability)
encoder_decoder_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

#  Callbacks (More Aggressive Overfitting Control)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, min_lr=1e-5, verbose=1)  # More responsive

#  Convert to NumPy and Reshape Input Data for GRU
X_train_encdec = np.array(X_train).reshape((X_train.shape[0], X_train.shape[1], 1))
X_test_encdec = np.array(X_test).reshape((X_test.shape[0], X_test.shape[1], 1))

#  Train the Model
encoder_decoder_model.fit(X_train_encdec, y_train, epochs=100, batch_size=32, validation_data=(X_test_encdec, y_test),
                          callbacks=[early_stopping, reduce_lr])

#  Predictions
y_train_pred = encoder_decoder_model.predict(X_train_encdec)
y_test_pred = encoder_decoder_model.predict(X_test_encdec)

#  Convert Predictions to Binary Labels
y_train_pred = (y_train_pred > 0.5).astype(int)
y_test_pred = (y_test_pred > 0.5).astype(int)

#  Compute F1-Score
f1_train_encdec = f1_score(y_train, y_train_pred)
f1_test_encdec = f1_score(y_test, y_test_pred)

#  Compute AUPRC
precision, recall, _ = precision_recall_curve(y_test, encoder_decoder_model.predict(X_test_encdec)[:, 0])
auprc = auc(recall, precision)

#  Print Results
print(f"F1-Score (Train): {f1_train_encdec:.4f}")
print(f"F1-Score (Test): {f1_test_encdec:.4f}")
print(f"AUPRC: {auprc:.4f}")

print(classification_report(y_test, y_test_pred))

#  Plot Confusion Matrix
plt.figure(figsize=(4, 3))
sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix - Encoder-Decoder Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()